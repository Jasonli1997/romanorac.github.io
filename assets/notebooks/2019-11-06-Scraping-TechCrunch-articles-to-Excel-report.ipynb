{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping TechCrunch articles to Excel report\n",
    "\n",
    "In this blog post, we are going to scrape the latest TechCrunch articles and save them to an Excel report using BeautifulSoup, a Python library for scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version==3.7.3\n",
      "pandas==0.25.0\n",
      "bs4==4.8.0\n",
      "requests==2.21.0\n",
      "xlsxwriter==1.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"python version==%s\" % python_version())\n",
    "print(\"pandas==%s\" % pd.__version__)\n",
    "print(\"bs4==%s\" % bs4.__version__)\n",
    "print(\"requests==%s\" % requests.__version__)\n",
    "print(\"xlsxwriter==%s\" % xlsxwriter.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deciding what to scrape\n",
    "\n",
    "Firstly, we need to decide what we would like to scrape from the website. \n",
    "In our example, these are the latest articles from TechCrunch (marked with a red square on the image below).\n",
    "For each article, we would like to scrape a title, short content and its URL.\n",
    "\n",
    "Let's inspect the HTML in a web browser (right-click on a webpage -> inspect) and look for a pattern in HTML elements that formats the latest articles (marked with a blue square on the image below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](scrape_techcrunch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles are marked with elements: `<article class=\"post-block post-block--image post-block--unread\">`.\n",
    "When we drill down, we get to the elements with a title, content and an attribute with the URL to the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrapping\n",
    "\n",
    "Now that we identified the elements we would like to parse, let's fetch the TechCrunch webpage and parse it with BeautifulSoup's HTML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://techcrunch.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](parse_techcrunch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the parsed output above, we see that instead of `<article>` elements, there are `<div>` elements - you can read more about why this happens in the [answer on StackOverflow](https://stackoverflow.com/questions/39101335/why-isnt-the-html-i-get-from-beautifulsoup-the-same-as-the-one-i-see-when-i-ins).\n",
    "\n",
    "To parse articles from the parsed HTML, we need to define HTML elements: \n",
    " - parent element of an article is marked with `div` and attributes `class=\"post-block post-block--image post-block--unread\"`\n",
    " - title and url are in a separate block from the content: `class=\"post-block__title__link\"`, `class=\"post-block__content\"` respectevly.\n",
    "\n",
    "The code below parses article's title, short content and a URL and it appends them to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles, article_contents, article_hrefs = [], [], []\n",
    "\n",
    "for tag in soup.findAll(\"div\", {\"class\": \"post-block post-block--image post-block--unread\"}):\n",
    "    tag_header = tag.find(\"a\", {\"class\": \"post-block__title__link\"})\n",
    "    tag_content = tag.find(\"div\", {\"class\": \"post-block__content\"})\n",
    "\n",
    "    article_title = tag_header.get_text().strip()\n",
    "    article_href = tag_header[\"href\"]\n",
    "    article_content = tag_content.get_text().strip()\n",
    "\n",
    "    article_titles.append(article_title)\n",
    "    article_contents.append(article_content)\n",
    "    article_hrefs.append(article_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an Excel report\n",
    "\n",
    "We have parsed the content of the webpage. Now let's save it to an Excel file.\n",
    "Pandas DataFrame enables us to create an Excel report with few commands. \n",
    "Let's create a pandas DataFrame from the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"title\": article_titles, \"content\": article_contents, \"href\": article_hrefs})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Francisco smokes Juul’s hopes by voting to...</td>\n",
       "      <td>Voters in San Francisco have resoundingly reje...</td>\n",
       "      <td>https://techcrunch.com/2019/11/06/san-francisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neo4j introduces new cloud service to simplify...</td>\n",
       "      <td>Neo4j, a popular graph database, is available ...</td>\n",
       "      <td>https://techcrunch.com/2019/11/06/neo4j-introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China’s Didi to relaunch Hitch carpooling serv...</td>\n",
       "      <td>Chinese ride-hailing firm Didi Chuxing said to...</td>\n",
       "      <td>https://techcrunch.com/2019/11/06/didi-hitch-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GoCardless partners with TransferWise to bring...</td>\n",
       "      <td>GoCardless, the London fintech that makes it e...</td>\n",
       "      <td>https://techcrunch.com/2019/11/06/gocardless-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72 hours left for early bird passes to Disrupt...</td>\n",
       "      <td>Did you know that the cuckoo clock originated ...</td>\n",
       "      <td>https://techcrunch.com/2019/11/06/72-hours-lef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  San Francisco smokes Juul’s hopes by voting to...   \n",
       "1  Neo4j introduces new cloud service to simplify...   \n",
       "2  China’s Didi to relaunch Hitch carpooling serv...   \n",
       "3  GoCardless partners with TransferWise to bring...   \n",
       "4  72 hours left for early bird passes to Disrupt...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Voters in San Francisco have resoundingly reje...   \n",
       "1  Neo4j, a popular graph database, is available ...   \n",
       "2  Chinese ride-hailing firm Didi Chuxing said to...   \n",
       "3  GoCardless, the London fintech that makes it e...   \n",
       "4  Did you know that the cuckoo clock originated ...   \n",
       "\n",
       "                                                href  \n",
       "0  https://techcrunch.com/2019/11/06/san-francisc...  \n",
       "1  https://techcrunch.com/2019/11/06/neo4j-introd...  \n",
       "2  https://techcrunch.com/2019/11/06/didi-hitch-c...  \n",
       "3  https://techcrunch.com/2019/11/06/gocardless-t...  \n",
       "4  https://techcrunch.com/2019/11/06/72-hours-lef...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_adjust_excel_columns(worksheet, df):\n",
    "    for idx, col in enumerate(df):  # loop through all columns\n",
    "        series = df[col]\n",
    "        max_len = (\n",
    "            max(\n",
    "                (\n",
    "                    series.astype(str).map(len).max(),  # len of largest item\n",
    "                    len(str(series.name)),  # len of column name/header\n",
    "                )\n",
    "            )\n",
    "            + 1\n",
    "        )  # adding a little extra space\n",
    "        worksheet.set_column(idx, idx, max_len)  # set column width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Excel columns are not auto-adjusted, so we need to set the width of the columns (a maximum between column title and largest item in the column).\n",
    "The code below auto-adjusts columns and it creates an Excel file from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('TechCrunch_latest_news.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "auto_adjust_excel_columns(writer.sheets['Sheet1'], df)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](excel_report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blogpost, we scraped the latest articles from TechCrunch and save them in a format that can be used by non-developers.\n",
    "Python and its libraries enable us to achieve that with a few commands.\n",
    "Each website s different and it requires a bit of manual searching for the right elements to parse.\n",
    "\n",
    "Did you find this tutorial useful? Have any suggestions? Let me know in the comments below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
